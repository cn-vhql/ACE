# ACE Framework - Agentic Context Engineering

<div align="center">

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-production%20ready-brightgreen.svg)

[ğŸ“– Documentation](#-documentation) | [ğŸš€ Quick Start](#-quick-start) | [ğŸŒ Web Interface](#-web-interface) | [ğŸ“Š Performance](#-performance) | [âš™ï¸ Configuration](#-configuration)

</div>

A Python implementation of the ACE (Agentic Context Engineering) framework that enables self-improving large language models through evolving context. Equipped with a comprehensive Streamlit web interface for intuitive visualized management and interactive experience.

## ğŸŒŸ Project Highlights

- **ğŸ§  Intelligent Self-Improvement**: AI systems continuously optimize performance through iterative learning
- **ğŸ¨ Visual Interface**: Modern web interface for intuitive management of AI improvement processes
- **ğŸ“Š Real-time Monitoring**: Performance statistics, trend analysis, knowledge graph visualization
- **ğŸ”§ Highly Configurable**: Support for multiple LLM providers and custom parameters
- **ğŸ“ˆ Production Ready**: Robust error handling, comprehensive test coverage

## ğŸ“‹ Overview

The ACE framework treats context as a dynamically evolving playbook rather than static prompts. It enables large language models to continuously improve performance through modular processes of generation, reflection, and curation by accumulating, refining, and organizing strategies.

### ğŸ”„ Core Components

1. **Generator (Executor)**:
   - Generates complete reasoning trajectories for new queries
   - Exposes effective strategies and recurring failure patterns
   - Supports code generation, API calls, multi-step reasoning, etc.

2. **Reflector (Analyst)**:
   - Critically analyzes Generator's trajectories based on execution feedback
   - Extracts specific, reusable insights and improvement suggestions
   - Multi-round iterative optimization of insight quality
   - Tags context items by usefulness (helpful/harmful/neutral)

3. **Curator (Integrator)**:
   - Synthesizes Reflector's refined insights into compact delta entries
   - Integrates into existing context through lightweight non-LLM logic
   - Performs semantic embedding deduplication to avoid redundancy
   - Maintains knowledge structure integrity

### âœ¨ Core Features

- **ğŸ§© Modular Architecture**: Three specialized components work together with clear responsibilities
- **ğŸ“ˆ Incremental Learning**: Avoids context collapse through structured delta updates
- **ğŸ”„ Self-Improvement**: Learns from execution feedback without manual annotation supervision
- **ğŸ“š Comprehensive Context**: Preserves detailed domain knowledge rather than compression loss
- **âš¡ Cost-Efficient**: 86.9% average reduction in adaptation latency
- **ğŸŒ Web Interface**: Complete Streamlit visualized management platform
- **ğŸ“Š Data Visualization**: Knowledge graphs, heatmaps, trend analysis, etc.
- **ğŸ” Real-time Monitoring**: Performance metrics, success rates, improvement trajectories

## ğŸš€ Quick Start

### Requirements

- Python 3.11+
- Supported LLM API keys (ModelScope, OpenAI, or Anthropic)

### Installation

```bash
# Clone repository
git clone <repository-url>
cd ace

# Install basic dependencies
pip install -e .

# Install visualization dependencies (for web interface)
pip install plotly networkx pandas streamlit

# Or install all dependencies at once
pip install -e .[dev]
```

### ğŸŒ Web Interface (Recommended)

```bash
# Start web interface
python run_streamlit.py

# Then visit http://localhost:8501 in your browser
```

Web interface provides:
- ğŸ“Š Visualized dashboard and statistical monitoring
- ğŸ’¬ Interactive Q&A interface
- ğŸ“š Playbook visualized management
- ğŸ§ª Batch processing and performance evaluation
- âš™ï¸ Configuration management and monitoring

### ğŸ Quick Start with Command Line

```python
import asyncio
from ace import ACE
from ace.config_loader import get_ace_config

async def main():
    # Load configuration from file
    config = get_ace_config()

    # Initialize ACE
    ace = ACE(config)

    # Solve query
    query = "Write a Python function to calculate factorial of a number"
    trajectory, reflection = await ace.solve_query(query)

    print(f"Success: {trajectory.success}")
    print(f"Generated code:\n{trajectory.generated_code}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ—ï¸ Architecture Design

### ğŸ”„ Workflow

```
Query Input â†’ Generator (Generate Trajectory) â†’ Execute Code â†’ Reflector (Analyze & Reflect) â†’ Curator (Integrate & Update) â†’ Playbook Evolution
```

### ğŸ›ï¸ System Architecture Diagram

```mermaid
graph TB
    subgraph "External Interface Layer"
        USER[User Input]
        CONFIG[Config File config.yaml]
        API[LLM API Interface]
    end

    subgraph "ACE Framework Core"
        ACE[ACE Framework]

        subgraph "Core Components"
            GEN[Generator<br/>Executor]
            REF[Reflector<br/>Analyzer]
            CUR[Curator<br/>Integrator]
            LLM[LLM Client<br/>Unified LLM Client]
        end

        subgraph "Data Models"
            PB[Playbook<br/>Knowledge Base]
            BUL[Bullet<br/>Knowledge Item]
            TRAJ[Trajectory<br/>Reasoning Path]
            REFLECT[Reflection<br/>Analysis]
            DELTA[DeltaUpdate<br/>Incremental Update]
        end
    end

    subgraph "Execution Environment"
        EXEC[Code Executor<br/>Optional]
        RESULT[Execution Result]
    end

    %% Connections
    USER --> ACE
    CONFIG --> ACE
    ACE --> GEN
    ACE --> REF
    ACE --> CUR
    GEN --> LLM
    REF --> LLM
    CUR --> LLM
    LLM --> API

    GEN --> TRAJ
    REF --> REFLECT
    CUR --> DELTA

    PB --> BUL
    GEN --> PB
    REF --> PB
    CUR --> PB

    TRAJ --> EXEC
    EXEC --> RESULT
    RESULT --> REF

    style ACE fill:#e1f5fe
    style GEN fill:#f3e5f5
    style REF fill:#e8f5e8
    style CUR fill:#fff3e0
    style LLM fill:#fce4ec
```

### ğŸŒŠ Data Flow Diagram

```mermaid
flowchart TD
    START([Start]) --> QUERY{Receive Query}
    QUERY --> RETRIEVE[Retrieve Relevant Bullets from Playbook]
    RETRIEVE --> GENERATE[Generate Reasoning Trajectory]

    subgraph "Generation Phase"
        GENERATE --> LLM1[Call LLM to Generate]
        LLM1 --> TRAJ[Create Trajectory Object]
        TRAJ --> EXECUTE{Execute Code?}
        EXECUTE -->|Yes| CODE_RUN[Run Generated Code]
        EXECUTE -->|No| STORE_RESULT[Store Result]
        CODE_RUN --> STORE_RESULT
    end

    STORE_RESULT --> REFLECT[Generate Reflection Analysis]

    subgraph "Reflection Phase"
        REFLECT --> ANALYZE[Analyze Execution Result]
        ANALYZE --> LLM2[Call LLM for Reflection]
        LLM2 --> REF_OBJ[Create Reflection Object]
        REF_OBJ --> VALIDATE{Validate Field Types}
        VALIDATE -->|List Type| CONVERT[Convert to String]
        VALIDATE -->|String Type| SKIP[Skip Conversion]
        CONVERT --> REF_COMPLETE[Reflection Complete]
        SKIP --> REF_COMPLETE
    end

    REF_COMPLETE --> CURATE[Create Delta Update]

    subgraph "Curation Phase"
        CURATE --> EXTRACT[Extract Key Insights]
        EXTRACT --> LLM3[Call LLM for Curation]
        LLM3 --> DELTA[Create DeltaUpdate]
        DELTA --> APPLY[Apply to Playbook]
        APPLY --> DEDUPE[Semantic Deduplication]
        DEDUPE --> UPDATE_PB[Update Knowledge Base]
    end

    UPDATE_PB --> STATS[Update Statistics]
    STATS --> RETURN[Return Result]
    RETURN --> END([End])

    %% Feedback Loop
    UPDATE_PB -.->|Improve Knowledge Base| RETRIEVE

    style QUERY fill:#e3f2fd
    style REFLECT fill:#e8f5e8
    style CURATE fill:#fff3e0
    style VALIDATE fill:#ffebee
    style CONVERT fill:#f3e5f5
```

### ğŸ”— Data Model Relationship Diagram

```mermaid
erDiagram
    PLAYBOOK {
        datetime created_at
        datetime updated_at
        dict sections
        dict metadata
    }

    BULLET {
        string id
        string content
        string bullet_type
        string section
        int helpful_count
        int harmful_count
        datetime created_at
        datetime updated_at
        dict metadata
    }

    TRAJECTORY {
        string id
        string query
        list reasoning_steps
        string generated_code
        string execution_result
        boolean success
        string error_message
        list used_bullet_ids
        dict metadata
        datetime created_at
    }

    REFLECTION {
        string id
        string trajectory_id
        string reasoning
        string error_identification
        string root_cause_analysis
        string correct_approach
        string key_insight
        dict bullet_tags
        datetime created_at
    }

    DELTA_UPDATE {
        string id
        list operations
        string reasoning
        datetime created_at
    }

    %% Relationships
    PLAYBOOK ||--o{ BULLET : contains
    PLAYBOOK ||--o{ TRAJECTORY : influences
    TRAJECTORY ||--|| REFLECTION : generates
    REFLECTION ||--|| DELTA_UPDATE : creates
    DELTA_UPDATE ||--o{ BULLET : modifies

    %% Data Type Notes
    note right of BULLET : bullet_type can be:<br/>STRATEGY, INSIGHT, ERROR_PATTERN,<br/>API_GUIDELINE, VERIFICATION_CHECK,<br/>FORMULA, DOMAIN_KNOWLEDGE

    note right of REFLECTION : bullet_tags key-value pairs:<br/>bullet_id -> BULLET_TAG<br/>(HELPFUL, HARMFUL, NEUTRAL)
```

### ğŸ“Š Data Models Detail

```python
# Playbook - Container of knowledge
Playbook:
  - bullets: List[Bullet]     # List of knowledge items
  - sections: Dict[str, List[str]]  # Section organization
  - metadata: Dict[str, Any]  # Metadata

# Bullet - Single knowledge unit
Bullet:
  - content: str              # Knowledge content
  - bullet_type: BulletType   # Type (strategy/error/API guide, etc.)
  - section: str              # Section category
  - helpful_count: int        # Helpful usage count
  - harmful_count: int        # Harmful usage count

# Trajectory - Complete reasoning execution process
Trajectory:
  - query: str                # Original query
  - reasoning_steps: List[str] # Reasoning steps
  - generated_code: str       # Generated code
  - execution_result: str     # Execution result
  - success: bool             # Execution success

# Reflection - Analysis of trajectory
Reflection:
  - reasoning: str            # Analysis reasoning
  - key_insight: str          # Key insight
  - correct_approach: str     # Correct approach
  - bullet_tags: Dict         # Knowledge item tag updates
```

### ğŸ§© Core Concepts Explained

#### ğŸ“š Playbook
Dynamically evolving knowledge base organized by sections:
- **Strategy**: General problem-solving methods and approaches
- **Error Pattern**: Common errors and avoidance solutions
- **API Guideline**: Best practices for specific APIs
- **Verification Check**: Result validation steps
- **Formula**: Mathematical and computational formulas
- **Domain Knowledge**: Professional domain knowledge

#### ğŸ”« Bullets
Atomic knowledge units in the playbook:
- **Content**: Specific knowledge description
- **Type**: One of 7 predefined types
- **Section**: Knowledge category classification
- **Usefulness Statistics**: Track helpful/harmful usage counts
- **Metadata**: Creation time, update history, etc.

#### ğŸ”„ Delta Updates
Key mechanism to avoid context collapse:
- **Non-destructive**: Add new knowledge without rewriting existing content
- **Smart Merging**: Deduplication and semantic similarity detection
- **Size Control**: Prevent unlimited playbook growth
- **Version Tracking**: Record knowledge evolution history

## ğŸ’¡ Usage Examples

### ğŸ¯ Single Query Solution

```python
import asyncio
from ace import ACE
from ace.config_loader import get_ace_config

async def single_query_example():
    # Initialize ACE
    config = get_ace_config()
    ace = ACE(config)

    # Solve query, automatically update playbook
    trajectory, reflection = await ace.solve_query(
        "Write a quicksort algorithm and analyze its time complexity"
    )

    print(f"âœ… Execution successful: {trajectory.success}")
    print(f"ğŸ“ Generated code:\n{trajectory.generated_code}")
    print(f"ğŸ” Key insight: {reflection.key_insight}")

asyncio.run(single_query_example())
```

### ğŸ“ Batch Learning Training

```python
async def batch_training_example():
    config = get_ace_config()
    ace = ACE(config)

    # Training query set
    training_queries = [
        "Implement binary search algorithm",
        "Write linked list reversal function",
        "Create depth-first search algorithm",
        "Implement dynamic programming knapsack solution"
    ]

    # Offline adaptation training
    training_stats = await ace.offline_adaptation(
        training_queries,
        epochs=3
    )

    print(f"ğŸ“š Training completed")
    print(f"   Initial knowledge items: {training_stats['initial_playbook_size']}")
    print(f"   Final knowledge items: {training_stats['final_playbook_size']}")
    print(f"   Success rate: {training_stats['epoch_stats'][-1]['success_rate']:.2%}")

asyncio.run(batch_training_example())
```

### ğŸ”„ Online Adaptation Improvement

```python
async def online_adaptation_example():
    config = get_ace_config()
    ace = ACE(config)

    # Online learning with feedback
    trajectory, reflection = await ace.online_adaptation(
        "Optimize this database query performance",
        execution_feedback="Query timeout, need to add index"
    )

    print(f"ğŸ’¡ Improvement suggestion: {reflection.correct_approach}")
    print(f"ğŸ¯ Playbook updated: {len(ace.playbook.bullets)} knowledge items")

asyncio.run(online_adaptation_example())
```

### ğŸ“Š Performance Evaluation Test

```python
async def evaluation_example():
    config = get_ace_config()
    ace = ACE(config)

    # Test query set
    test_queries = [
        "Write a function to calculate Fibonacci sequence",
        "Implement string compression algorithm",
        "Create palindrome number checker function"
    ]

    # Performance evaluation (without updating playbook)
    results = await ace.evaluate_performance(test_queries)

    print(f"ğŸ“Š Evaluation results:")
    print(f"   Total queries: {results['total_queries']}")
    print(f"   Success rate: {results['success_rate']:.2%}")
    print(f"   Average confidence: {results['average_confidence']:.2%}")

    # Detailed results
    for i, result in enumerate(results['query_results']):
        print(f"   Query {i+1}: {'âœ…' if result['success'] else 'âŒ'} "
              f"(confidence: {result['confidence']:.2f})")

asyncio.run(evaluation_example())
```

## ğŸŒ Web Interface

### ğŸ¨ Feature Overview

ACE framework provides a fully-featured Streamlit web interface, allowing you to intuitively manage and monitor the AI system's self-improvement process.

### ğŸ“Š Main Features

#### ğŸ  Dashboard
- **Real-time Statistics**: Success rate, trajectory count, playbook size
- **Recent Activity**: Query history and execution status
- **Quick Operations**: Reset, save, download, etc.

#### ğŸ’¬ Interactive Q&A
- **Smart Queries**: Natural language input and processing
- **Real-time Execution**: Code generation and result display
- **Detailed Analysis**: Reasoning steps and reflection insights
- **Visualization**: Network graphs, timelines, charts

#### ğŸ“š Playbook Management
- **Knowledge Browsing**: Filter by type, section, tags
- **Visual Analysis**: Heatmaps, distribution charts, relationship networks
- **File Operations**: Upload, download, import, export
- **Real-time Statistics**: Knowledge distribution and evolution trends

#### ğŸ“Š Statistical Monitoring
- **Performance Trends**: Success rate changes over time
- **Query Analysis**: Activity heatmaps and frequency analysis
- **Comprehensive Metrics**: Detailed performance data display

#### ğŸ§ª Batch Processing
- **Batch Queries**: Process multiple tasks at once
- **Performance Evaluation**: Evaluate performance on test sets
- **Result Export**: Detailed evaluation reports

### ğŸš€ Starting Web Interface

```bash
# Basic start
python run_streamlit.py

# Custom port
python run_streamlit.py --port 8080

# Debug mode
python run_streamlit.py --debug

# View help
python run_streamlit.py --help
```

Visit http://localhost:8501 to start using

### ğŸ“± Interface Preview

- **ğŸ“Š Data Visualization**: Knowledge network graphs, heatmaps, trend analysis
- **ğŸ’¬ Smart Q&A**: Real-time query processing and result display
- **ğŸ“š Strategy Management**: Visualized knowledge browsing and editing
- **ğŸ“ˆ Performance Monitoring**: Real-time statistics and trend charts

## âš™ï¸ Configuration

### ğŸ“‹ Configuration File Structure

```yaml
# config.yaml
llm_provider:
  # Provider type: openai, anthropic, custom
  type: "custom"

  # ModelScope configuration
  custom:
    base_url: "https://api-inference.modelscope.cn/v1"
    api_key: "${MODELSCOPE_API_KEY}"  # Environment variable
    default_model: "Qwen/Qwen3-32B"

  # OpenAI alternative configuration
  openai:
    base_url: "https://api.openai.com/v1"
    api_key: "${OPENAI_API_KEY}"
    default_model: "gpt-4"

# Model parameter configuration
models:
  generator:
    model: "Qwen/Qwen3-32B"
    temperature: 0.7
    max_tokens: 4096
  reflector:
    model: "Qwen/Qwen3-32B"
    temperature: 0.3
    max_tokens: 4096
  curator:
    model: "Qwen/Qwen3-32B"
    temperature: 0.2
    max_tokens: 4096

# ACE framework parameters
ace:
  max_reflector_rounds: 3      # Maximum reflection rounds
  max_epochs: 5               # Maximum training epochs
  max_playbook_bullets: 1000   # Maximum playbook knowledge items
  similarity_threshold: 0.8    # Similarity threshold
  max_retrieved_bullets: 10    # Maximum retrieved knowledge items

# Execution settings
execution:
  timeout: 30                 # Code execution timeout (seconds)
  enable_execution: true       # Enable code execution
```

### ğŸ”§ Environment Variables

```bash
# ModelScope API key
export MODELSCOPE_API_KEY="your-modelscope-key"

# OpenAI API key (alternative)
export OPENAI_API_KEY="your-openai-key"

# Anthropic API key (alternative)
export ANTHROPIC_API_KEY="your-anthropic-key"
```

## ğŸ§ª Testing and Validation

### ğŸ§ª Running Tests

```bash
# Full functionality test
python test_streamlit.py

# Basic functionality test
python test_basic_streamlit.py

# Unit tests
python -m pytest tests/ -v

# Specific component test
python -m pytest tests/test_ace.py::TestACEFramework -v

# Test coverage
python -m pytest tests/ --cov=ace --cov-report=html
```

### ğŸ® Demo Examples

```bash
# Main program demo
python main.py

# Quick feature demo
python run_example.py demo

# Basic usage examples
python run_example.py basic

# Mathematical reasoning examples
python run_example.py math

# Code generation examples
python run_example.py code
```

## ğŸ“ Project Structure

```
ACE/
â”œâ”€â”€ ğŸ“„ README.md                    # Main project documentation
â”œâ”€â”€ ğŸ“„ README_EN.md                 # English documentation
â”œâ”€â”€ ğŸ“„ CLAUDE.md                   # Claude development guide
â”œâ”€â”€ ğŸ“„ STREAMLIT_README.md         # Web interface usage documentation
â”œâ”€â”€ ğŸ“„ STREAMLIT_SUMMARY.md        # Web interface implementation summary
â”œâ”€â”€ ğŸ“„ pyproject.toml              # Project configuration and dependencies
â”œâ”€â”€ ğŸ“„ config.yaml                 # Framework configuration file
â”œâ”€â”€ ğŸš€ main.py                     # Main program demo
â”œâ”€â”€ ğŸš€ run_example.py              # Example run script
â”œâ”€â”€ ğŸŒ streamlit_app.py            # Web interface main application
â”œâ”€â”€ ğŸŒ run_streamlit.py            # Web interface startup script
â”œâ”€â”€ ğŸ§ª test_streamlit.py           # Web interface test
â”œâ”€â”€ ğŸ§ª test_basic_streamlit.py     # Basic functionality test
â”œâ”€â”€ ğŸ“ ace/                        # Framework core code
â”‚   â”œâ”€â”€ ğŸ __init__.py             # Module exports
â”‚   â”œâ”€â”€ ğŸ§  ace_framework.py        # Main framework class
â”‚   â”œâ”€â”€ ğŸ¤– llm_client.py           # LLM client
â”‚   â”œâ”€â”€ âš¡ generator.py            # Generator component
â”‚   â”œâ”€â”€ ğŸ” reflector.py            # Reflector component
â”‚   â”œâ”€â”€ ğŸ“š curator.py              # Curator component
â”‚   â”œâ”€â”€ ğŸ“Š models.py               # Data models
â”‚   â”œâ”€â”€ âš™ï¸ config_loader.py         # Configuration loader
â”‚   â”œâ”€â”€ ğŸŒ streamlit_utils.py      # Web interface utilities
â”‚   â””â”€â”€ ğŸ“ examples/               # Usage examples
â”‚       â””â”€â”€ ğŸ basic_usage.py      # Basic usage example
â”œâ”€â”€ ğŸ“ tests/                      # Test code
â”‚   â”œâ”€â”€ ğŸ __init__.py
â”‚   â”œâ”€â”€ ğŸ§ª test_ace.py             # Framework core test
â”‚   â””â”€â”€ ğŸ§ª test_*.py               # Other component tests
â”œâ”€â”€ ğŸ“ doc/                        # Documentation directory
â”‚   â””â”€â”€ ğŸ“„ Agentic Context Engineering.pdf  # Research paper
â””â”€â”€ ğŸ“ .venv/                      # Virtual environment
```

## ğŸ“Š Performance Metrics

### ğŸ† Core Performance

Based on experimental results from the original paper:

- **ğŸ¤– Agent Tasks**: Average improvement of **10.6%** over strong baselines
- **ğŸ¯ Domain-Specific Tasks**: Average improvement of **8.6%**
- **âš¡ Adaptation Latency**: Reduction of **86.9%**
- **ğŸ”„ Self-Improvement**: Works effectively without annotation supervision

### ğŸ“ˆ Usage Statistics

Performance under typical usage scenarios:

- **ğŸ“ Code Generation**: 85-95% success rate
- **ğŸ”§ Problem Solving**: 80-90% success rate
- **ğŸ§® Mathematical Reasoning**: 75-85% success rate
- **ğŸ’¡ Creative Tasks**: 70-80% success rate

## ğŸ”¬ Research Background

### ğŸ“– Foundation Paper

This implementation is based on the paper **"Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models"** by Zhang Qizheng et al. (arXiv:2510.04618, 2025).

### ğŸ§  Core Insights

1. **Simplicity Bias Problem**:
   - Traditional prompt optimization produces overly concise instructions
   - Loses valuable domain-specific insights
   - Difficult to handle complex multi-step tasks

2. **Context Collapse Phenomenon**:
   - Iterative rewriting leads to knowledge compression
   - Performance sharply declines over time
   - Difficult to maintain knowledge integrity

3. **Evolutionary Strategy Solution**:
   - Treat context as dynamically evolving knowledge base
   - Preserve detailed domain knowledge and strategies
   - Avoid information loss through incremental updates

### ğŸ¯ Innovative Contributions

- **ğŸ“š Playbook Architecture**: Structured knowledge organization
- **ğŸ”„ Three-Component Collaboration**: Generation-reflection-integration cycle
- **âš¡ Incremental Learning**: Non-destructive knowledge updates
- **ğŸ§  Self-Improvement**: Learning without external supervision

## âš ï¸ Limitations and Considerations

### ğŸš« Technical Limitations

1. **API Costs**: Requires calling LLM provider APIs, incurring costs
2. **Quality Dependency**: Performance depends on base model capabilities and training data
3. **Feedback Requirements**: Works best with reliable execution feedback
4. **Domain Specificity**: Most beneficial for knowledge-intensive tasks

### ğŸ”’ Security Considerations

1. **Code Execution**: Framework executes generated code, requires secure environment
2. **API Keys**: Properly secure API keys to avoid leakage
3. **Data Privacy**: Be mindful of data privacy when processing queries
4. **Resource Consumption**: Heavy usage may consume computational resources

### ğŸ’¡ Best Practices

1. **Start Simple**: Begin with basic features before exploring advanced capabilities
2. **Monitor Costs**: Track API call counts and expenses
3. **Regular Backups**: Save important playbook states
4. **Performance Tuning**: Adjust parameters based on specific tasks

## ğŸ¤ Contributing Guidelines

### ğŸ› ï¸ Development Environment Setup

```bash
# Clone repository
git clone <repository-url>
cd ace

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or .venv\Scripts\activate  # Windows

# Install development dependencies
pip install -e .[dev]

# Run tests to ensure environment is working
python test_streamlit.py
```

### ğŸ“ Contribution Process

1. **ğŸ´ Fork Repository** and create feature branch
2. **ğŸ§ª Write Tests** to ensure functionality correctness
3. **âœ… Pass All Tests** maintain code quality
4. **ğŸ“– Update Documentation** including new feature descriptions
5. **ğŸ“¤ Submit Pull Request** wait for review

### ğŸ› Reporting Issues

Please use GitHub Issues to report problems, including:
- Detailed error information
- Reproduction steps
- Environment information
- Expected behavior

## ğŸ“ Support and Contact

### ğŸ“§ Contact Information

- **Email**: yl_zhangqiang@foxmail.com
- **GitHub Issues**: Project issue tracking
- **Documentation**: See `STREAMLIT_README.md` for detailed usage guide

### ğŸ“š Related Resources

- [Original Paper](https://arxiv.org/abs/2510.04618)
- [Streamlit Official Documentation](https://docs.streamlit.io/)
- [Pydantic Data Validation](https://pydantic-docs.helpmanual.io/)
- [Plotly Visualization](https://plotly.com/python/)

## ğŸ“„ Citation

If you use this framework in your research, please cite the original paper:

```bibtex
@article{zhang2025agentic,
  title={Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models},
  author={Zhang, Qizheng and Hu, Changran and Upasani, Shubhangi and others},
  journal={arXiv preprint arXiv:2510.04618},
  year={2025}
}
```

---

<div align="center">

**ğŸ§  ACE Framework - Agentic Context Engineering**

Enabling AI systems to continuously evolve through self-improvement

[â­ Give us a Star](../../) | [ğŸ› Report Issues](../../issues) | [ğŸ’¡ Feature Suggestions](../../issues/new)

</div>

---

**âš ï¸ Disclaimer**: This is an independent implementation of the ACE framework and is not affiliated with the original paper authors. The project is for research and educational purposes only.